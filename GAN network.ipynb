{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://github.com/uclaacmai/Generative-Adversarial-Network-Tutorial/blob/master/Generative%20Adversarial%20Networks%20Tutorial.ipynb\n",
    "# https://github.com/T-Almeida/GAN-study\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(r\"C:\\Users\\Admin\\Documents\\ALIYA\\1. Aliya Thesis\\DATASET\\1. NSL KDD\\mywork\\nslKDDNoheaders.csv\", header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>duration</th>\n",
       "      <th>protocol_type</th>\n",
       "      <th>service</th>\n",
       "      <th>flag</th>\n",
       "      <th>src_bytes</th>\n",
       "      <th>dst_bytes</th>\n",
       "      <th>land</th>\n",
       "      <th>wrong_fragment</th>\n",
       "      <th>urgent</th>\n",
       "      <th>hot</th>\n",
       "      <th>...</th>\n",
       "      <th>dst_host_srv_count</th>\n",
       "      <th>dst_host_same_srv_rate</th>\n",
       "      <th>dst_host_diff_srv_rate</th>\n",
       "      <th>dst_host_same_src_port_rate</th>\n",
       "      <th>dst_host_srv_diff_host_rate</th>\n",
       "      <th>dst_host_serror_rate</th>\n",
       "      <th>dst_host_srv_serror_rate</th>\n",
       "      <th>dst_host_rerror_rate</th>\n",
       "      <th>dst_host_srv_rerror_rate</th>\n",
       "      <th>outcome</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>491</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>25</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>146</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>26</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>232</td>\n",
       "      <td>8153</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>255</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.01</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>199</td>\n",
       "      <td>420</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>255</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 42 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   duration  protocol_type  service  flag  src_bytes  dst_bytes  land  \\\n",
       "0         0              1        1     1        491          0     0   \n",
       "1         0              2        2     1        146          0     0   \n",
       "2         0              1        3     2          0          0     0   \n",
       "3         0              1        4     1        232       8153     0   \n",
       "4         0              1        4     1        199        420     0   \n",
       "\n",
       "   wrong_fragment  urgent  hot  ...  dst_host_srv_count  \\\n",
       "0               0       0    0  ...                  25   \n",
       "1               0       0    0  ...                   1   \n",
       "2               0       0    0  ...                  26   \n",
       "3               0       0    0  ...                 255   \n",
       "4               0       0    0  ...                 255   \n",
       "\n",
       "   dst_host_same_srv_rate  dst_host_diff_srv_rate  \\\n",
       "0                    0.17                    0.03   \n",
       "1                    0.00                    0.60   \n",
       "2                    0.10                    0.05   \n",
       "3                    1.00                    0.00   \n",
       "4                    1.00                    0.00   \n",
       "\n",
       "   dst_host_same_src_port_rate  dst_host_srv_diff_host_rate  \\\n",
       "0                         0.17                         0.00   \n",
       "1                         0.88                         0.00   \n",
       "2                         0.00                         0.00   \n",
       "3                         0.03                         0.04   \n",
       "4                         0.00                         0.00   \n",
       "\n",
       "   dst_host_serror_rate  dst_host_srv_serror_rate  dst_host_rerror_rate  \\\n",
       "0                  0.00                      0.00                  0.05   \n",
       "1                  0.00                      0.00                  0.00   \n",
       "2                  1.00                      1.00                  0.00   \n",
       "3                  0.03                      0.01                  0.00   \n",
       "4                  0.00                      0.00                  0.00   \n",
       "\n",
       "   dst_host_srv_rerror_rate  outcome  \n",
       "0                      0.00        1  \n",
       "1                      0.00        1  \n",
       "2                      0.00        0  \n",
       "3                      0.01        1  \n",
       "4                      0.00        1  \n",
       "\n",
       "[5 rows x 42 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns = [\n",
    "    'duration',\n",
    "    'protocol_type',\n",
    "    'service',\n",
    "    'flag',\n",
    "    'src_bytes',\n",
    "    'dst_bytes',\n",
    "    'land',\n",
    "    'wrong_fragment',\n",
    "    'urgent',\n",
    "    'hot',\n",
    "    'num_failed_logins',\n",
    "    'logged_in',\n",
    "    'num_compromised',\n",
    "    'root_shell',\n",
    "    'su_attempted',\n",
    "    'num_root',\n",
    "    'num_file_creations',\n",
    "    'num_shells',\n",
    "    'num_access_files',\n",
    "    'num_outbound_cmds',\n",
    "    'is_host_login',\n",
    "    'is_guest_login',\n",
    "    'count',\n",
    "    'srv_count',\n",
    "    'serror_rate',\n",
    "    'srv_serror_rate',\n",
    "    'rerror_rate',\n",
    "    'srv_rerror_rate',\n",
    "    'same_srv_rate',\n",
    "    'diff_srv_rate',\n",
    "    'srv_diff_host_rate',\n",
    "    'dst_host_count',\n",
    "    'dst_host_srv_count',\n",
    "    'dst_host_same_srv_rate',\n",
    "    'dst_host_diff_srv_rate',\n",
    "    'dst_host_same_src_port_rate',\n",
    "    'dst_host_srv_diff_host_rate',\n",
    "    'dst_host_serror_rate',\n",
    "    'dst_host_srv_serror_rate',\n",
    "    'dst_host_rerror_rate',\n",
    "    'dst_host_srv_rerror_rate',\n",
    "    'outcome'\n",
    "]\n",
    "\n",
    "# display 5 rows\n",
    "df[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_columns = df.columns.drop('outcome')\n",
    "x = df[x_columns].values\n",
    "dummies = pd.get_dummies(df['outcome']) # Classification\n",
    "outcomes = dummies.columns\n",
    "num_classes = len(outcomes)\n",
    "y = dummies.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Create a test/train split.  25% test\n",
    "# Split into train/test\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    x, y, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18894, 41)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define some functions that will help us with creating CNNs in Tensorflow\n",
    "\n",
    "def conv2d(x, W):\n",
    "    return tf.nn.conv2d(input=x, filter=W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "def avg_pool_2x2(x):\n",
    "    return tf.nn.avg_pool(x, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator(x_train, reuse=False):\n",
    "    with tf.variable_scope('discriminator') as scope:\n",
    "        if (reuse):\n",
    "            tf.get_variable_scope().reuse_variables()\n",
    "        #First Conv and Pool Layers\n",
    "        W_conv1 = tf.get_variable('d_wconv1', [5, 5, 1, 8], initializer=tf.truncated_normal_initializer(stddev=0.02))\n",
    "        b_conv1 = tf.get_variable('d_bconv1', [8], initializer=tf.constant_initializer(0))\n",
    "        h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)\n",
    "        h_pool1 = avg_pool_2x2(h_conv1)\n",
    "\n",
    "        #Second Conv and Pool Layers\n",
    "        W_conv2 = tf.get_variable('d_wconv2', [5, 5, 8, 16], initializer=tf.truncated_normal_initializer(stddev=0.02))\n",
    "        b_conv2 = tf.get_variable('d_bconv2', [16], initializer=tf.constant_initializer(0))\n",
    "        h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)\n",
    "        h_pool2 = avg_pool_2x2(h_conv2)\n",
    "\n",
    "        #First Fully Connected Layer\n",
    "        W_fc1 = tf.get_variable('d_wfc1', [7 * 7 * 16, 32], initializer=tf.truncated_normal_initializer(stddev=0.02))\n",
    "        b_fc1 = tf.get_variable('d_bfc1', [32], initializer=tf.constant_initializer(0))\n",
    "        h_pool2_flat = tf.reshape(h_pool2, [-1, 7*7*16])\n",
    "        h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)\n",
    "\n",
    "        #Second Fully Connected Layer\n",
    "        W_fc2 = tf.get_variable('d_wfc2', [32, 1], initializer=tf.truncated_normal_initializer(stddev=0.02))\n",
    "        b_fc2 = tf.get_variable('d_bfc2', [1], initializer=tf.constant_initializer(0))\n",
    "\n",
    "        #Final Layer\n",
    "        y_conv=(tf.matmul(h_fc1, W_fc2) + b_fc2)\n",
    "    return y_conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator(z, batch_size, z_dim, reuse=False):\n",
    "    with tf.variable_scope('generator') as scope:\n",
    "        if (reuse):\n",
    "            tf.get_variable_scope().reuse_variables()\n",
    "        g_dim = 64 #Number of filters of first layer of generator \n",
    "        c_dim = 1 #Color dimension of output (MNIST is grayscale, so c_dim = 1 for us)\n",
    "        s = 28 #Output size of the image\n",
    "        s2, s4, s8, s16 = int(s/2), int(s/4), int(s/8), int(s/16) #We want to slowly upscale the image, so these values will help\n",
    "                                                                  #make that change gradual.\n",
    "\n",
    "        h0 = tf.reshape(z, [batch_size, s16+1, s16+1, 25])\n",
    "        h0 = tf.nn.relu(h0)\n",
    "        #Dimensions of h0 = batch_size x 2 x 2 x 25\n",
    "\n",
    "        #First DeConv Layer\n",
    "        output1_shape = [batch_size, s8, s8, g_dim*4]\n",
    "        W_conv1 = tf.get_variable('g_wconv1', [5, 5, output1_shape[-1], int(h0.get_shape()[-1])], \n",
    "                                  initializer=tf.truncated_normal_initializer(stddev=0.1))\n",
    "        b_conv1 = tf.get_variable('g_bconv1', [output1_shape[-1]], initializer=tf.constant_initializer(.1))\n",
    "        H_conv1 = tf.nn.conv2d_transpose(h0, W_conv1, output_shape=output1_shape, \n",
    "                                         strides=[1, 2, 2, 1], padding='SAME') + b_conv1\n",
    "        H_conv1 = tf.contrib.layers.batch_norm(inputs = H_conv1, center=True, scale=True, is_training=True, scope=\"g_bn1\")\n",
    "        H_conv1 = tf.nn.relu(H_conv1)\n",
    "        #Dimensions of H_conv1 = batch_size x 3 x 3 x 256\n",
    "\n",
    "        #Second DeConv Layer\n",
    "        output2_shape = [batch_size, s4 - 1, s4 - 1, g_dim*2]\n",
    "        W_conv2 = tf.get_variable('g_wconv2', [5, 5, output2_shape[-1], int(H_conv1.get_shape()[-1])], \n",
    "                                  initializer=tf.truncated_normal_initializer(stddev=0.1))\n",
    "        b_conv2 = tf.get_variable('g_bconv2', [output2_shape[-1]], initializer=tf.constant_initializer(.1))\n",
    "        H_conv2 = tf.nn.conv2d_transpose(H_conv1, W_conv2, output_shape=output2_shape, \n",
    "                                         strides=[1, 2, 2, 1], padding='SAME') + b_conv2\n",
    "        H_conv2 = tf.contrib.layers.batch_norm(inputs = H_conv2, center=True, scale=True, is_training=True, scope=\"g_bn2\")\n",
    "        H_conv2 = tf.nn.relu(H_conv2)\n",
    "        #Dimensions of H_conv2 = batch_size x 6 x 6 x 128\n",
    "\n",
    "        #Third DeConv Layer\n",
    "        output3_shape = [batch_size, s2 - 2, s2 - 2, g_dim*1]\n",
    "        W_conv3 = tf.get_variable('g_wconv3', [5, 5, output3_shape[-1], int(H_conv2.get_shape()[-1])], \n",
    "                                  initializer=tf.truncated_normal_initializer(stddev=0.1))\n",
    "        b_conv3 = tf.get_variable('g_bconv3', [output3_shape[-1]], initializer=tf.constant_initializer(.1))\n",
    "        H_conv3 = tf.nn.conv2d_transpose(H_conv2, W_conv3, output_shape=output3_shape, \n",
    "                                         strides=[1, 2, 2, 1], padding='SAME') + b_conv3\n",
    "        H_conv3 = tf.contrib.layers.batch_norm(inputs = H_conv3, center=True, scale=True, is_training=True, scope=\"g_bn3\")\n",
    "        H_conv3 = tf.nn.relu(H_conv3)\n",
    "        #Dimensions of H_conv3 = batch_size x 12 x 12 x 64\n",
    "\n",
    "        #Fourth DeConv Layer\n",
    "        output4_shape = [batch_size, s, s, c_dim]\n",
    "        W_conv4 = tf.get_variable('g_wconv4', [5, 5, output4_shape[-1], int(H_conv3.get_shape()[-1])], \n",
    "                                  initializer=tf.truncated_normal_initializer(stddev=0.1))\n",
    "        b_conv4 = tf.get_variable('g_bconv4', [output4_shape[-1]], initializer=tf.constant_initializer(.1))\n",
    "        H_conv4 = tf.nn.conv2d_transpose(H_conv3, W_conv4, output_shape=output4_shape, \n",
    "                                         strides=[1, 2, 2, 1], padding='VALID') + b_conv4\n",
    "        H_conv4 = tf.nn.tanh(H_conv4)\n",
    "        #Dimensions of H_conv4 = batch_size x 28 x 28 x 1\n",
    "\n",
    "    return H_conv4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"generator\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "g_input (InputLayer)         [(None, 100)]             0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 128)               12928     \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 41)                5289      \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 41)                0         \n",
      "=================================================================\n",
      "Total params: 18,217\n",
      "Trainable params: 18,217\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import Model,Input\n",
    "from tensorflow.keras.layers import Dense,Activation,LeakyReLU, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "#Dimention of random vector using for sampling\n",
    "z = 100\n",
    "\n",
    "\n",
    "g_input = Input(shape=[z], name=\"g_input\")\n",
    "H = Dense(128)(g_input)\n",
    "H = Activation('relu')(H)\n",
    "#H = Dense(256)(g_input)\n",
    "#H = Activation('relu')(H)\n",
    "H = Dense(41)(H)\n",
    "g_output = Activation('sigmoid')(H)\n",
    "\"\"\"\n",
    "#More advancer network\n",
    "g_input = Input(shape=[z])\n",
    "H = Dense(256)(g_input)\n",
    "H = LeakyReLU(alpha=0.2)(H)\n",
    "H = BatchNormalization(momentum=0.8)(H)\n",
    "H = Dense(512)(H)\n",
    "H = LeakyReLU(alpha=0.2)(H)\n",
    "H = BatchNormalization(momentum=0.8)(H)\n",
    "H = Dense(1024)(H)\n",
    "H = LeakyReLU(alpha=0.2)(H)\n",
    "H = BatchNormalization(momentum=0.8)(H)\n",
    "H = Dense(mnist_flat_size)(H)\n",
    "g_output = Activation('tanh')(H)\n",
    "\"\"\"\n",
    "generator = Model(g_input,g_output, name=\"generator\")\n",
    "#No need to compile because he will use train with adversarial method\n",
    "generator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"discriminator\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "d_input (InputLayer)         [(None, 41)]              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 128)               5376      \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 129       \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 5,505\n",
      "Trainable params: 5,505\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "d_optimizer = Adam(lr=0.001)\n",
    "\n",
    "\n",
    "d_input = Input(shape=[41],  name=\"d_input\")\n",
    "H = Dense(128)(d_input)\n",
    "H = Activation('relu')(H)\n",
    "H = Dense(1)(H)\n",
    "d_output = Activation('sigmoid')(H)\n",
    "\n",
    "\"\"\"\n",
    "d_input = Input(shape=[mnist_flat_size],  name=\"d_input\")\n",
    "H = Dense(512)(d_input)\n",
    "H = LeakyReLU(alpha=0.2)(H)\n",
    "H = Dense(128)(H)\n",
    "H = LeakyReLU(alpha=0.2)(H)\n",
    "H = Dense(1)(H)\n",
    "d_output = Activation('sigmoid')(H)\n",
    "\"\"\"\n",
    "discriminator = Model(d_input,d_output, name=\"discriminator\")\n",
    "discriminator.compile(loss='binary_crossentropy', optimizer=d_optimizer)\n",
    "discriminator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"generator_adversarial\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gan_z_input (InputLayer)     [(None, 100)]             0         \n",
      "_________________________________________________________________\n",
      "generator (Functional)       (None, 41)                18217     \n",
      "_________________________________________________________________\n",
      "discriminator (Functional)   (None, 1)                 5505      \n",
      "=================================================================\n",
      "Total params: 23,722\n",
      "Trainable params: 18,217\n",
      "Non-trainable params: 5,505\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def make_trainable(net, val=True):\n",
    "    net.trainable = val\n",
    "    for l in net.layers:\n",
    "        l.trainable = val\n",
    "\n",
    "# Freeze weights in the discriminator for adversarial training\n",
    "make_trainable(discriminator, False)\n",
    "\n",
    "gan_optimizer = Adam(lr=0.001)\n",
    "\n",
    "# Build stacked GAN model to train the generator\n",
    "gan_input = Input(shape=[z], name=\"gan_z_input\")\n",
    "gan_fake_samples = generator(gan_input)\n",
    "gan_output = discriminator(gan_fake_samples)\n",
    "\n",
    "gan = Model(gan_input, gan_output, name=\"generator_adversarial\")\n",
    "gan.compile(loss='binary_crossentropy', optimizer=gan_optimizer)\n",
    "gan.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# https://www.wouterbulten.nl/blog/tech/getting-started-with-generative-adversarial-networks/\n",
    "from keras.models import Sequential,Input, Model\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv1D, GlobalMaxPooling1D\n",
    "from keras import optimizers\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "\n",
    "def discriminator():\n",
    "    \n",
    "    net = Sequential()\n",
    "    input_shape = (121, 1)\n",
    "    dropout_prob = 0.4\n",
    "\n",
    "    net.add(Conv1D(64, strides=2, input_shape=input_shape, padding='same'))\n",
    "    net.add(LeakyReLU())\n",
    "    \n",
    "    net.add(Conv2D(128, strides=2, padding='same'))\n",
    "    net.add(LeakyReLU())\n",
    "    net.add(Dropout(dropout_prob))\n",
    "    \n",
    "    net.add(Conv2D(256, strides=2, padding='same'))\n",
    "    net.add(LeakyReLU())\n",
    "    net.add(Dropout(dropout_prob))\n",
    "    \n",
    "    net.add(Conv2D(512, strides=1, padding='same'))\n",
    "    net.add(LeakyReLU())\n",
    "    net.add(Dropout(dropout_prob))\n",
    "    \n",
    "    net.add(Flatten())\n",
    "    net.add(Dense(1))\n",
    "    net.add(Activation('sigmoid'))\n",
    "    \n",
    "    return net\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator():\n",
    "    \n",
    "    net = Sequential()\n",
    "    dropout_prob = 0.4\n",
    "    \n",
    "    net.add(Dense(7*256, input_dim=100))\n",
    "    net.add(BatchNormalization(momentum=0.9))\n",
    "    net.add(LeakyReLU())\n",
    "    net.add(Reshape((7,256)))\n",
    "    net.add(Dropout(dropout_prob))\n",
    "    \n",
    "    net.add(UpSampling1D())\n",
    "    net.add(Conv1D(128, padding='same'))\n",
    "    net.add(BatchNormalization(momentum=0.9))\n",
    "    net.add(LeakyReLU())\n",
    "    \n",
    "    net.add(UpSampling1D())\n",
    "    net.add(Conv1D(64, padding='same'))\n",
    "    net.add(BatchNormalization(momentum=0.9))\n",
    "    net.add(LeakyReLU())\n",
    "    \n",
    "    net.add(Conv1D(32, padding='same'))\n",
    "    net.add(BatchNormalization(momentum=0.9))\n",
    "    net.add(LeakyReLU())\n",
    "    \n",
    "    net.add(Conv1D(1, padding='same'))\n",
    "    net.add(Activation('sigmoid'))\n",
    "    \n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim_discriminator = optimizers.RMSprop(lr=0.0008, clipvalue=1.0, decay=1e-10)\n",
    "model_discriminator = Sequential()\n",
    "model_discriminator.add(discriminator)\n",
    "model_discriminator.compile(loss='binary_crossentropy', optimizer=optim_discriminator, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim_adversarial = Adam(lr=0.0004, clipvalue=1.0, decay=1e-10)\n",
    "model_adversarial = Sequential()\n",
    "# model_adversarial.add(net_generator)\n",
    "\n",
    "# Disable layers in discriminator\n",
    "for layer in net_discriminator.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# model_adversarial.add(net_discriminator)\n",
    "model_adversarial.compile(loss='binary_crossentropy', optimizer=optim_adversarial, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the standalone discriminator model using GAN training hacks \n",
    "def define_discriminator(in_shape=(128,1)):\n",
    "    model = Sequential()\n",
    "    # input layer with image size of 128x128, since its a colored image it has 3 channels\n",
    "    model.add(Conv1D(16, 3, padding='same', input_shape=in_shape))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    # downsample to 64x64 using strides of 2,2 and use of LeakyReLU\n",
    "    model.add(Conv1D(8, 3, strides=2, padding='same'))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    # downsample to 32x32\n",
    "    model.add(Conv1D(16, 3, strides=2, padding='same'))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    # downsample to 16x16\n",
    "    model.add(Conv1D(8, 3, strides=2, padding='same'))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    # now the image size is down to 16 x 16\n",
    "    # classifier \n",
    "    model.add(Flatten())\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    # compile model learning rate is higher than generator 2e-3\n",
    "    # use adam optimizer \n",
    "    opt = Adam(lr=0.0002, beta_1=0.5)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "    return model\n",
    " \n",
    "# define the standalone generator model using GAN training hacks\n",
    "def define_generator(latent_dim):\n",
    "    model = Sequential()\n",
    "    # foundation for 16x16 image\n",
    "    n_nodes = 256 * 16\n",
    "    model.add(Dense(n_nodes, input_dim=latent_dim))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Reshape((16, 256)))\n",
    "    # upsample to 32x32, use of strides and LeakyReLU\n",
    "    model.add(Conv1DTranspose(128, 4, strides=2, padding='same'))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    # upsample to 64x64\n",
    "    model.add(Conv1DTranspose(128,4, strides=2, padding='same'))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    # upsample to 128x128\n",
    "    model.add(Conv1DTranspose(128, 4, strides=2, padding='same'))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    # output layer, use of tanh as per hacks\n",
    "    model.add(Conv1D(3,3, activation='tanh', padding='same'))\n",
    "    return model\n",
    "\n",
    "# combined generator and discriminator model, for updating the generator\n",
    "# this is alogical GAN model using above defined generator and discriminator \n",
    "def define_gan(g_model, d_model):\n",
    "    # make weights in the discriminator not trainable\n",
    "    d_model.trainable = False\n",
    "    # initialize with a sequential model\n",
    "    model = Sequential()\n",
    "    # generator and discriminator are initialized later in the code to g_model and d_model respectively \n",
    "    # add the generator\n",
    "    model.add(g_model)\n",
    "    # add the discriminator\n",
    "    model.add(d_model)\n",
    "    # compile model\n",
    "    # use of adam optimizer, learning rate is lower than discriminator 2e-4\n",
    "    opt = Adam(lr=0.00002, beta_1=0.5)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=opt)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read images from df_4 dataframe into x_train\n",
    "x_train = []\n",
    "def load_real_samples():\n",
    "    for f, breed in tqdm(df_4.values):\n",
    "        try:\n",
    "            img = image.load_img(('/storage/train/{}'.format(f)), target_size=(128, 128))\n",
    "            # convert to float32\n",
    "            arr1 = image.img_to_array(img, dtype = 'float32')\n",
    "            # scale images to [-1,1] from [0,255]\n",
    "            arr = (arr1 - 127.5) / 127.5\n",
    "            x_train.append(arr)\n",
    "        except:\n",
    "            pass\n",
    "    return x_train\n",
    "    \n",
    "# select real samples\n",
    "# images are loaded into dataset variable using load_real_samples() function\n",
    "# below function will randomly select images from dataset and spit out X and y \n",
    "# X contains images, y contains lables. (model is not trained with labels as all the images are of same label)\n",
    "def generate_real_samples(dataset, n_samples):\n",
    "    # choose random instances\n",
    "    ix = randint(0, len(dataset), n_samples)\n",
    "    # generate 'real' class labels (1)\n",
    "    y = ones((n_samples, 1))   \n",
    "    # retrieve selected images\n",
    "    X = []  \n",
    "    f = 0 \n",
    "    for f in range (len(ix)):\n",
    "        X.append(dataset[ix[f]])\n",
    "    #print(len(X))\n",
    "    return X, y \n",
    "    \n",
    "# Use Gaussian Latent Space\n",
    "# generate points in latent space as input for the generator\n",
    "# The latent space defines the shape and distribution of the input to the generator model used to generate new images.\n",
    "def generate_latent_points(latent_dim, n_samples):\n",
    "\t# generate points in the latent space\n",
    "\tx_input = randn(latent_dim * n_samples)\n",
    "\t# reshape into a batch of inputs for the network\n",
    "\tx_input = x_input.reshape(n_samples, latent_dim)\n",
    "\treturn x_input\n",
    " \n",
    "# use the generator to generate n fake examples, with class labels\n",
    "def generate_fake_samples(g_model, latent_dim, n_samples):\n",
    "\t# generate points in latent space\n",
    "\tx_input = generate_latent_points(latent_dim, n_samples)\n",
    "\t# predict outputs\n",
    "\tX = g_model.predict(x_input)\n",
    "\t# create 'fake' class labels (0)\n",
    "\ty = zeros((n_samples, 1))\n",
    "\treturn X, y"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
